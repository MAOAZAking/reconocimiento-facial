<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contenido Protegido</title>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
        body { font-family: sans-serif; display: flex; flex-direction: column; align-items: center; margin: 0; background-color: #333; color: white; }
        #videoContainer {
            position: relative;
            width: 720px;
            height: 560px;
            margin-top: 20px;
            border: 3px solid white;
            border-radius: 10px;
            overflow: hidden;
            transition: all 0.5s ease-in-out;
        }
        video, canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; }
        #status { margin-top: 15px; font-size: 24px; font-weight: bold; height: 40px; }
        #protectedContent {
            display: none;
            margin-top: 30px;
            padding: 40px;
            border: 2px dashed #4CAF50;
            background-color: #444;
            text-align: center;
            width: 100%;
            box-sizing: border-box;
        }

        /* --- Estilos para la vista desbloqueada --- */
        body.unlocked > h1,
        body.unlocked > p {
            display: none;
        }
        body.unlocked #videoContainer {
            position: fixed;
            top: 20px;
            left: 20px;
            width: 240px;
            height: 180px;
            z-index: 100;
            margin-top: 0;
        }
        body.unlocked #protectedContent {
            display: block;
            margin-top: 0;
        }
    </style>
</head>
<body>
    <h1>Paso 2: Desbloqueo de Contenido</h1>
    <p>Mira a la cámara. Si tu rostro es el autorizado, el contenido aparecerá.</p>
    <div id="status">Cargando...</div>
    <div id="videoContainer">
        <video id="video" width="720" height="560" autoplay muted playsinline></video>
        <canvas id="canvas"></canvas>
    </div>

    <div id="protectedContent">
        <h2>¡Acceso Concedido!</h2>
        <p>Este es el contenido secreto que solo tú puedes ver.</p>
        <img src="https://via.placeholder.com/400x200.png?text=Contenido+Secreto" alt="Contenido Secreto">
    </div>

    <script>
        const video = document.getElementById('video');
        const statusEl = document.getElementById('status');
        const protectedContent = document.getElementById('protectedContent');
        const bodyEl = document.body;
        const MODEL_URL = 'models';
        let faceMatcher = null;

        // --- Variables para la detección de vida (Liveness Detection) ---
        let livenessVerified = false; // Flag para saber si ya pasó la prueba
        let blinkCounter = 0; // Contador de frames con ojos cerrados
        let totalBlinks = 0; // Contador de parpadeos completados
        const EYE_AR_THRESH = 0.3; // Aumentamos umbral (0.3) para detectar más fácil el ojo cerrado.
        const EYE_AR_CONSEC_FRAMES = 1; // Reducimos a 1 frame porque la detección web es lenta y puede perder frames intermedios.
        const REQUIRED_BLINKS = 2; // Número de parpadeos requeridos.

        async function start() {
            let labeledFaceDescriptors = [];

            try {
                // Ahora se lee de un archivo físico, no de localStorage.
                // 1. Cargar el manifiesto que lista todos los archivos de rostros.
                const manifestResponse = await fetch('rostros_manifest.json');
                if (!manifestResponse.ok) throw new Error("No se encontró 'rostros_manifest.json'.");
                const faceFiles = await manifestResponse.json();

                if (!Array.isArray(faceFiles) || faceFiles.length === 0) {
                    throw new Error("El manifiesto está vacío o no es válido.");
                }

                // 2. Cargar cada archivo de rostro listado en el manifiesto.
                const faceDataPromises = faceFiles.map(file => fetch(file).then(res => res.json()));
                const allFacesData = await Promise.all(faceDataPromises);

                // 3. Procesar todos los datos de rostros y crear los descriptores etiquetados.
                labeledFaceDescriptors = allFacesData.map(faceData => {
                    // Cada archivo contiene un objeto { label, descriptors }
                    const descriptors = faceData.descriptors.map(d => new Float32Array(d));
                    return new faceapi.LabeledFaceDescriptors(faceData.label, descriptors);
                });

            } catch (error) {
                statusEl.innerText = `Error: ${error.message}. Por favor, genera los archivos desde la página de registro.`;
                setTimeout(() => window.location.href = 'registro_facial.html', 4000);
                return;
            }

            statusEl.innerText = "Cargando modelos de IA...";
            await Promise.all([
                faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
                faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
                faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL)
            ]);
            
            faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.55);

            statusEl.innerText = "Iniciando cámara...";
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
                video.srcObject = stream;
            } catch (err) {
                console.error(err);
                statusEl.innerText = "Error al acceder a la cámara.";
            }
        }

        // Función para calcular el "Eye Aspect Ratio" (EAR)
        function getEyeAspectRatio(eyeLandmarks) {
            // eyeLandmarks son 6 puntos (p1 a p6)
            const dist = (pA, pB) => Math.sqrt(Math.pow(pA.x - pB.x, 2) + Math.pow(pA.y - pB.y, 2));

            // Distancias verticales
            const verticalDist1 = dist(eyeLandmarks[1], eyeLandmarks[5]);
            const verticalDist2 = dist(eyeLandmarks[2], eyeLandmarks[4]);

            // Distancia horizontal
            const horizontalDist = dist(eyeLandmarks[0], eyeLandmarks[3]);

            // Calcular EAR
            return (verticalDist1 + verticalDist2) / (2.0 * horizontalDist);
        }

        video.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(video);
            document.getElementById('videoContainer').append(canvas);

            const displaySize = { width: video.clientWidth, height: video.clientHeight };
            faceapi.matchDimensions(canvas, displaySize);

            statusEl.innerText = "Escaneando rostro...";

            // Usamos una función recursiva en lugar de setInterval para evitar solapamientos y mejorar rendimiento
            const detectLoop = async () => {
                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
                
                if (!livenessVerified) {
                    // SEGURIDAD: Si estamos en prueba de vida, el contenido DEBE estar oculto.
                    bodyEl.classList.remove('unlocked');

                    // --- LÓGICA DE DETECCIÓN DE VIDA (PARPADEO) ---
                    // Para la prueba de vida, solo necesitamos los landmarks, es más rápido.
                    // Bajamos minConfidence a 0.3 para no perder el rostro cuando los ojos se cierran (que a veces confunde al modelo)
                    const detection = await faceapi.detectSingleFace(video, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.3 })).withFaceLandmarks();

                    if (detection) {
                        const landmarks = detection.landmarks;
                        const leftEye = landmarks.getLeftEye();
                        const rightEye = landmarks.getRightEye();
                        const leftEAR = getEyeAspectRatio(leftEye);
                        const rightEAR = getEyeAspectRatio(rightEye);
                        const ear = (leftEAR + rightEAR) / 2.0;

                        if (ear < EYE_AR_THRESH) {
                            blinkCounter++;
                        } else {
                            // Si los ojos están abiertos, comprobamos si se completó un parpadeo
                            if (blinkCounter >= EYE_AR_CONSEC_FRAMES) {
                                totalBlinks++;
                            }
                            blinkCounter = 0; // En cualquier caso, reseteamos el contador de frames cerrados
                        }

                        statusEl.innerText = `Prueba de vida: Parpadea (${totalBlinks}/${REQUIRED_BLINKS})`;

                        if (totalBlinks >= REQUIRED_BLINKS) {
                            livenessVerified = true;
                            statusEl.innerText = "Prueba superada. Verificando identidad...";
                        }
                    } else {
                        // Si no hay rostro, reseteamos contadores y mensaje
                        statusEl.innerText = "Buscando un rostro...";
                        totalBlinks = 0;
                        blinkCounter = 0;
                    }
                } else {
                    // --- LÓGICA DE RECONOCIMIENTO FACIAL ---
                    // Ahora sí, corremos la detección completa con descriptores
                    const detection = await faceapi.detectSingleFace(video).withFaceLandmarks().withFaceDescriptor();

                    if (detection && faceMatcher) {
                        const bestMatch = faceMatcher.findBestMatch(detection.descriptor);
                        if (bestMatch.label !== 'unknown') {
                            bodyEl.classList.add('unlocked');
                            statusEl.innerText = `Hola ${bestMatch.label}, Acceso Concedido`;
                            document.getElementById('videoContainer').style.borderColor = '#4CAF50';
                        } else {
                            // No autorizado, reseteamos la prueba de vida para un nuevo intento
                            bodyEl.classList.remove('unlocked');
                            statusEl.innerText = "Acceso Denegado";
                            document.getElementById('videoContainer').style.borderColor = '#f44336';
                            livenessVerified = false;
                            totalBlinks = 0;
                            blinkCounter = 0;
                        }
                    } else {
                        // Si se pierde el rostro después de la prueba, reseteamos todo
                        bodyEl.classList.remove('unlocked');
                        statusEl.innerText = "Buscando un rostro...";
                        document.getElementById('videoContainer').style.borderColor = 'white';
                        livenessVerified = false;
                        totalBlinks = 0;
                        blinkCounter = 0;
                    }
                }
                // Llamar al siguiente frame lo antes posible
                setTimeout(detectLoop, 0);
            };
            detectLoop();
        });

        start();
    </script>
</body>
</html>